{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import glob\n",
    "from process_jtwc import *\n",
    "from itertools import product\n",
    "import pymongo\n",
    "import requests, zipfile, io\n",
    "import csv\n",
    "import pdb\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JTWC\n",
    "\n",
    "From https://www.metoc.navy.mil/jtwc/jtwc.html?best-tracks\n",
    "\n",
    "# Download files for certain years and regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download JTWC data for certain years\n",
    "\n",
    "years = [ x for x in range(2000, 2001+1)]\n",
    "regions = ['bio', 'bsh', 'bwp']\n",
    "def get_tc_by_year_region(year=2018, region='bio'):\n",
    "    url = make_url(year, region)\n",
    "    resp = requests.get(url)\n",
    "    if not resp.status_code // 100 == 2:\n",
    "        return \"Error: Unexpected response {}\".format(resp)\n",
    "    z = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    file_path = make_file_path(year, region)\n",
    "    z.extractall(file_path)\n",
    "\n",
    "def make_file_path(year, region, base='/storage/hurricane'):\n",
    "    filePath = os.path.join(base, region, str(year))\n",
    "    if not os.path.exists(filePath):\n",
    "        os.makedirs(filePath)\n",
    "    return filePath\n",
    "\n",
    "def make_url(year=2018, region='bio'):\n",
    "    return f'https://www.metoc.navy.mil/jtwc/products/best-tracks/{year}/{year}s-{region}/{region}{year}.zip'\n",
    "\n",
    "def download_jtwc_files(years, regions, base='/storage/hurricane'):\n",
    "    for year, region in product(years, regions):\n",
    "        make_file_path(year, region, base)\n",
    "        get_tc_by_year_region(year, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_jtwc_files(years, regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert files into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lst = []\n",
    "dr = glob.glob('/storage/hurricane/*/*/*.dat') # location of files taken from website.\n",
    "ulist = []\n",
    "for fn in dr:\n",
    "#     print(fn)\n",
    "    raw = pd.read_table(fn, header=None, delimiter=',', usecols=range(11))\n",
    "    df_raw = convert_df(raw)\n",
    "    ulist = ulist + df_raw.ID.unique().tolist()\n",
    "    df_lst = df_lst + df_raw.to_dict(orient='records')\n",
    "df = pd.DataFrame(df_lst)\n",
    "df['source'] = 'JTWC'\n",
    "df = df.applymap(lambda x: x.replace(' ', '') if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename({\"ID\":'_id', \"LONG\": 'lon', 'press': 'pres', 'SEASON':'year'}, axis=1)\n",
    "df.columns = [col.lower() for col in df.columns]\n",
    "df.year = df.year.astype(np.int64)\n",
    "df.time = df.time.astype(np.int64)\n",
    "df.lat = df.lat.astype(np.float64)\n",
    "df.lon = df.lon.astype(np.float64)\n",
    "df['geoLocation'] = [ {\"type\": \"point\", \"coordinates\": [lng, lat]} for lng, lat in df[['lon', 'lat']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_empty(x):\n",
    "    if isinstance(x, str):\n",
    "        return x == ''\n",
    "    elif isinstance(x, float):\n",
    "        return np.isnan(x)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "clean_dict_keys = lambda my_dict: list(filter(lambda k: not is_empty(my_dict[k]), my_dict))\n",
    "def clean_dict(my_dict):\n",
    "    new_dict = dict()\n",
    "    keys = clean_dict_keys(my_dict)\n",
    "    for key in keys:\n",
    "        new_dict[key] = my_dict[key]\n",
    "    return new_dict\n",
    "\n",
    "def make_docs(df):\n",
    "    docs = []\n",
    "    keys = ['_id', 'name', 'num', 'source']\n",
    "    key_types = {'_id': str, 'name':str , 'num': int, 'source': str}\n",
    "    cols = [col for col in df.columns if col not in keys]\n",
    "    for _id, df_id in df.groupby(['_id']):\n",
    "        df_id.shape\n",
    "        doc = {}\n",
    "        for key in keys:\n",
    "            tpe = key_types[key]\n",
    "            assert len(df_id[key].unique()) == 1, 'nondistinct id'\n",
    "            doc[key] = df_id[key].astype(tpe).iloc[0]\n",
    "            if key == 'num':\n",
    "                doc[key] = int(doc[key])\n",
    "        traj_data = df_id[cols].to_dict(orient='records')\n",
    "        traj_data = [clean_dict(x) for x in traj_data]\n",
    "        year = int(df_id.year.iloc[0])\n",
    "        doc['year'] = year\n",
    "        doc['startDate'] = df_id.timestamp.min()\n",
    "        doc['endDate'] = df_id.timestamp.max()\n",
    "        doc['traj_data'] = df_id[cols].to_dict(orient='records')\n",
    "        doc['_id'] = doc['_id'] + '_' + doc['source']\n",
    "        if doc['name'] == 'UNNAMED':\n",
    "            del doc['name']\n",
    "        docs.append(doc)\n",
    "    docs = [clean_dict(x) for x in docs]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jtwc_docs = make_docs(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HURDAT2\n",
    "from https://www.nhc.noaa.gov/data/#hurdat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacific_filename = '/storage/hurricane/hurdat2/pacific.csv'\n",
    "atlantic_filename = '/storage/hurricane/hurdat2/atlantic.csv'\n",
    "stormStartCh = ['EP', 'CP', 'AL']\n",
    "\n",
    "def make_trop_cyc_list(filename):\n",
    "    startIdx = []\n",
    "    tcs = []\n",
    "    with open(filename) as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',')\n",
    "        for idx, row in enumerate(spamreader):\n",
    "            if len(row) == 0:\n",
    "                continue\n",
    "            if row[0][0:2] in stormStartCh:\n",
    "                startIdx.append(idx)\n",
    "                _id = row[0]\n",
    "                name= row[1]\n",
    "                num = row[2]\n",
    "                storm = [_id, name, num]\n",
    "            else:\n",
    "                tc = storm + row\n",
    "                tcs.append(tc[0:11])\n",
    "    return tcs\n",
    "\n",
    "pacific_tcs = make_trop_cyc_list(pacific_filename)\n",
    "atlantic_tcs = make_trop_cyc_list(atlantic_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['_id', 'name', 'num', 'date', 'time', 'l', 'class', 'lat', 'lon', 'wind', 'pres']\n",
    "def convert_lat_lon(strL, postiveDir='N'):\n",
    "    L = float(strL[:-1].replace(' ', ''))\n",
    "    if not postiveDir in strL:\n",
    "        L *= -1\n",
    "    return L\n",
    "\n",
    "def convert_time(time):\n",
    "    hour = (int(time)/100) %12 \n",
    "    return hour\n",
    "\n",
    "def make_cyc_df(tcs):\n",
    "    df = pd.DataFrame(tcs, columns=cols)\n",
    "    df['year'] = df.date.apply(lambda x: int(x[0:4]))\n",
    "    df = df[df['year'] >= 2000]\n",
    "    df = df.dropna(axis=0, how='any', subset=['lat', 'lon'])\n",
    "    df = df.applymap(lambda x: x.replace(' ', '') if isinstance(x, str) else x)\n",
    "    df.lon = df.lon.apply(lambda lon: convert_lat_lon(lon, 'E')).astype(np.float64)\n",
    "    df.lat = df.lat.apply(lambda lat: convert_lat_lon(lat, 'N')).astype(np.float64)\n",
    "    df.pres = df.pres.astype(np.int64)\n",
    "    df.pres = df.pres.replace(-999, np.nan)\n",
    "    df.wind = df.wind.astype(np.int64)\n",
    "    df.num = df.num.astype(np.int64)\n",
    "    df.date = df.date.apply(lambda x: x[0:4] + '-' + x[4:6] + '-' + x[6:8])\n",
    "    datetimes = df.date.values + ' ' + df.time.values\n",
    "    df['timestamp'] = pd.to_datetime(datetimes, format='%Y-%m-%d %H%M')\n",
    "    df['source'] = 'HURDAT2'\n",
    "    df['geoLocation'] = [ {\"type\": \"point\", \"coordinates\": [lng, lat]} for lng, lat in df[['lon', 'lat']].values]\n",
    "    df.time = df.time.astype(np.int64)\n",
    "    return df\n",
    "\n",
    "df_pacific = make_cyc_df(pacific_tcs)\n",
    "df_atlantic = make_cyc_df(atlantic_tcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hurdat2_docs = make_docs(df_pacific) + make_docs(df_atlantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>name</th>\n",
       "      <th>num</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>l</th>\n",
       "      <th>class</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>wind</th>\n",
       "      <th>pres</th>\n",
       "      <th>year</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>geoLocation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28126</th>\n",
       "      <td>EP142018</td>\n",
       "      <td>LANE</td>\n",
       "      <td>64</td>\n",
       "      <td>2018-08-13</td>\n",
       "      <td>1200</td>\n",
       "      <td></td>\n",
       "      <td>LO</td>\n",
       "      <td>10.9</td>\n",
       "      <td>-114.7</td>\n",
       "      <td>20</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-08-13 12:00:00</td>\n",
       "      <td>HURDAT2</td>\n",
       "      <td>{'type': 'point', 'coordinates': [-114.7, 10.9]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28127</th>\n",
       "      <td>EP142018</td>\n",
       "      <td>LANE</td>\n",
       "      <td>64</td>\n",
       "      <td>2018-08-13</td>\n",
       "      <td>1800</td>\n",
       "      <td></td>\n",
       "      <td>LO</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-115.7</td>\n",
       "      <td>20</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-08-13 18:00:00</td>\n",
       "      <td>HURDAT2</td>\n",
       "      <td>{'type': 'point', 'coordinates': [-115.7, 11.0]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28128</th>\n",
       "      <td>EP142018</td>\n",
       "      <td>LANE</td>\n",
       "      <td>64</td>\n",
       "      <td>2018-08-14</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>LO</td>\n",
       "      <td>11.1</td>\n",
       "      <td>-116.6</td>\n",
       "      <td>25</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-08-14 00:00:00</td>\n",
       "      <td>HURDAT2</td>\n",
       "      <td>{'type': 'point', 'coordinates': [-116.6, 11.1]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28129</th>\n",
       "      <td>EP142018</td>\n",
       "      <td>LANE</td>\n",
       "      <td>64</td>\n",
       "      <td>2018-08-14</td>\n",
       "      <td>600</td>\n",
       "      <td></td>\n",
       "      <td>LO</td>\n",
       "      <td>11.1</td>\n",
       "      <td>-117.4</td>\n",
       "      <td>25</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-08-14 06:00:00</td>\n",
       "      <td>HURDAT2</td>\n",
       "      <td>{'type': 'point', 'coordinates': [-117.4, 11.1]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28130</th>\n",
       "      <td>EP142018</td>\n",
       "      <td>LANE</td>\n",
       "      <td>64</td>\n",
       "      <td>2018-08-14</td>\n",
       "      <td>1200</td>\n",
       "      <td></td>\n",
       "      <td>LO</td>\n",
       "      <td>11.1</td>\n",
       "      <td>-118.4</td>\n",
       "      <td>25</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-08-14 12:00:00</td>\n",
       "      <td>HURDAT2</td>\n",
       "      <td>{'type': 'point', 'coordinates': [-118.4, 11.1]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            _id  name  num        date  time l class   lat    lon  wind  \\\n",
       "28126  EP142018  LANE   64  2018-08-13  1200      LO  10.9 -114.7    20   \n",
       "28127  EP142018  LANE   64  2018-08-13  1800      LO  11.0 -115.7    20   \n",
       "28128  EP142018  LANE   64  2018-08-14     0      LO  11.1 -116.6    25   \n",
       "28129  EP142018  LANE   64  2018-08-14   600      LO  11.1 -117.4    25   \n",
       "28130  EP142018  LANE   64  2018-08-14  1200      LO  11.1 -118.4    25   \n",
       "\n",
       "         pres  year           timestamp   source  \\\n",
       "28126  1009.0  2018 2018-08-13 12:00:00  HURDAT2   \n",
       "28127  1009.0  2018 2018-08-13 18:00:00  HURDAT2   \n",
       "28128  1009.0  2018 2018-08-14 00:00:00  HURDAT2   \n",
       "28129  1009.0  2018 2018-08-14 06:00:00  HURDAT2   \n",
       "28130  1009.0  2018 2018-08-14 12:00:00  HURDAT2   \n",
       "\n",
       "                                            geoLocation  \n",
       "28126  {'type': 'point', 'coordinates': [-114.7, 10.9]}  \n",
       "28127  {'type': 'point', 'coordinates': [-115.7, 11.0]}  \n",
       "28128  {'type': 'point', 'coordinates': [-116.6, 11.1]}  \n",
       "28129  {'type': 'point', 'coordinates': [-117.4, 11.1]}  \n",
       "28130  {'type': 'point', 'coordinates': [-118.4, 11.1]}  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lane = df_pacific[(df_pacific['name'] == 'LANE') & (df_pacific['year'] == 2018)]\n",
    "df_lane.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = make_docs(df_lane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add docs to mongoDB database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collection(dbName, collectionName, init_collection):\n",
    "    dbUrl = 'mongodb://localhost:27017/'\n",
    "    client = pymongo.MongoClient(dbUrl)\n",
    "    db = client[dbName]\n",
    "    coll = db[collectionName]\n",
    "    coll = init_collection(coll)\n",
    "    return coll\n",
    "\n",
    "def init_tc_collection(coll):\n",
    "    coll.create_index([('name', pymongo.DESCENDING)])\n",
    "    coll.create_index([('startDate', pymongo.DESCENDING)])\n",
    "    coll.create_index([('endDate', pymongo.DESCENDING)])\n",
    "    coll.create_index([('startDate', pymongo.DESCENDING), ('endDate', pymongo.DESCENDING)])\n",
    "    return coll\n",
    "\n",
    "def init_tc_traj_collection(coll):\n",
    "    return coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "BulkWriteError",
     "evalue": "batch op errors occurred",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBulkWriteError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-73ca2cb68bc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcoll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectionName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_tc_collection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#coll.drop()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjtwc_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mcoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhurdat2_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/argo/lib/python3.6/site-packages/pymongo/collection.py\u001b[0m in \u001b[0;36minsert_many\u001b[0;34m(self, documents, ordered, bypass_document_validation, session)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0mblk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Bulk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mordered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbypass_document_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m         \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_concern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mInsertManyResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minserted_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_concern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macknowledged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/argo/lib/python3.6/site-packages/pymongo/bulk.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, write_concern, session)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_no_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_concern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/argo/lib/python3.6/site-packages/pymongo/bulk.py\u001b[0m in \u001b[0;36mexecute_command\u001b[0;34m(self, generator, write_concern, session)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfull_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"writeErrors\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfull_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"writeConcernErrors\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0m_raise_bulk_write_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfull_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/argo/lib/python3.6/site-packages/pymongo/bulk.py\u001b[0m in \u001b[0;36m_raise_bulk_write_error\u001b[0;34m(full_result)\u001b[0m\n\u001b[1;32m    138\u001b[0m         full_result[\"writeErrors\"].sort(\n\u001b[1;32m    139\u001b[0m             key=lambda error: error[\"index\"])\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mBulkWriteError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBulkWriteError\u001b[0m: batch op errors occurred"
     ]
    }
   ],
   "source": [
    "#insert docs\n",
    "dbName='argo'\n",
    "collectionName = 'tc'\n",
    "coll = create_collection(dbName, collectionName, init_tc_collection)\n",
    "coll.drop()\n",
    "coll.insert_many(jtwc_docs)\n",
    "coll.insert_many(hurdat2_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f8015eb4a08>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll = create_collection('argo-express-test', 'tc', init_tc_collection)\n",
    "coll.drop()\n",
    "coll.insert_many(test_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add southern ocean storms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_so_cyc_df(df):\n",
    "    df['year'] = df.timestamp.apply(lambda x: x.year)\n",
    "    df['time'] = df.timestamp.apply(lambda x: x.hour*100)\n",
    "    df['num'] = 0\n",
    "    df['class'] = 'Southern Hemisphere Cylone'\n",
    "    df['pres'] = 0\n",
    "    df['wind'] = 0\n",
    "    df.lon = df.lon.astype(np.float64)\n",
    "    df.lat = df.lat.astype(np.float64)\n",
    "    df.date = df.timestamp.apply(lambda x: datetime.strftime(x, '%y-%m-%d'))\n",
    "    df['source'] = 'Priestley'\n",
    "    df['geoLocation'] = [ {\"type\": \"point\", \"coordinates\": [lng, lat]} for lng, lat in df[['lon', 'lat']].values]\n",
    "    return df\n",
    "\n",
    "def parse_file(file):\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "    sdxs = []\n",
    "    storm = False\n",
    "    for idx, line in enumerate(lines):\n",
    "        if 'TRACK_ID' in line:\n",
    "            sdx = idx\n",
    "            sdxs.append(idx)\n",
    "    linesIdx = [ (sdxs[idx], sdxs[idx+1]) for idx in range(len(sdxs)-1)]\n",
    "    storms = [lines[start:end] for start, end in linesIdx]    \n",
    "    return storms\n",
    "\n",
    "def create_storm_df(storm, basedate, name):\n",
    "    #create base_id\n",
    "    trackNumber = storm.pop(0).split()[1]\n",
    "    pointNumber = storm.pop(0).split()[1]\n",
    "    _id = '_'.join((name, year, trackNumber, pointNumber))\n",
    "\n",
    "    #parse thru storm lines for time, lat, lon\n",
    "    rows = []\n",
    "    for line in storm:\n",
    "        rows.append([float(x) for x in line.split()])\n",
    "    df = pd.DataFrame(rows, columns=('timestamp', 'lon', 'lat', 'intensity'))\n",
    "    #convert timestamp\n",
    "    df['timestamp'] = df['timestamp'].apply(lambda x: basedate + timedelta(hours=deltaT*x))\n",
    "    df['_id'] = _id\n",
    "    df['name'] = name\n",
    "    return df\n",
    "\n",
    "def parse_filename(file):\n",
    "    year = file.split('/')[-1].split('_')[1][0:4]\n",
    "    name = '_'.join(file.split('/')[4:6])\n",
    "    basedate = datetime.strptime(year + '-01-01', '%Y-%m-%d')\n",
    "    return year, name, basedate\n",
    "\n",
    "def insert_one_by_one(docs, coll):\n",
    "    for doc in docs:\n",
    "        nDup = 0\n",
    "        try:    \n",
    "            coll.insert_one(doc)\n",
    "        except pymongo.helpers.DuplicateKeyError:\n",
    "            nDup += 1\n",
    "            pass\n",
    "        except Exception as err:\n",
    "            pdb.set_trace()\n",
    "            print(err)   \n",
    "    print(f'{nDup} duplicates found')\n",
    "\n",
    "def insert_many_tc_docs(docs, coll):\n",
    "    try:\n",
    "        coll.insert_many(docs)\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        print('trying to add one at a time')\n",
    "        insert_one_by_one(docs, coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'argo-express-test'), 'tc')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/anaconda3/envs/argo/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserting 393 docs\n",
      "inserting 405 docs\n",
      "inserting 383 docs\n",
      "inserting 398 docs\n",
      "inserting 404 docs\n",
      "inserting 433 docs\n",
      "inserting 426 docs\n",
      "inserting 386 docs\n",
      "inserting 446 docs\n",
      "inserting 375 docs\n",
      "inserting 379 docs\n",
      "inserting 375 docs\n",
      "inserting 377 docs\n",
      "inserting 394 docs\n",
      "inserting 408 docs\n",
      "inserting 411 docs\n",
      "inserting 406 docs\n",
      "inserting 404 docs\n",
      "inserting 436 docs\n",
      "inserting 432 docs\n",
      "inserting 445 docs\n",
      "inserting 452 docs\n",
      "inserting 472 docs\n",
      "inserting 452 docs\n",
      "inserting 463 docs\n",
      "inserting 461 docs\n",
      "inserting 457 docs\n",
      "inserting 457 docs\n",
      "inserting 456 docs\n",
      "inserting 446 docs\n",
      "inserting 441 docs\n",
      "inserting 447 docs\n",
      "inserting 465 docs\n",
      "inserting 473 docs\n",
      "inserting 459 docs\n",
      "inserting 448 docs\n",
      "inserting 453 docs\n",
      "inserting 461 docs\n",
      "attempted 16279 inserts\n"
     ]
    }
   ],
   "source": [
    "sonFiles = glob.glob('/storage/hurricane/priestley/SON/SH_FILT/ERA5_20*_TRACKS_FILTERED_neg')\n",
    "djfFiles = glob.glob('/storage/hurricane/priestley/DJF/SH_FILT/ERA5_20*_TRACKS_FILTERED_neg')\n",
    "deltaT = 6 #hours\n",
    "files = djfFiles + sonFiles\n",
    "\n",
    "# dbName='argo'\n",
    "# collectionName = 'tc'\n",
    "# coll = create_collection(dbName, collectionName, init_tc_collection)\n",
    "print(f'{len(files)} files')\n",
    "docs = []\n",
    "totalDocs = 0\n",
    "for file in files:\n",
    "    storms = parse_file(file)\n",
    "    #get base date\n",
    "    year, name, basedate = parse_filename(file)\n",
    "    for storm in storms:\n",
    "        df = create_storm_df(storm, basedate, name)\n",
    "        df = make_so_cyc_df(df)\n",
    "        docs += make_docs(df)\n",
    "    totalDocs += len(docs)\n",
    "    print(f'inserting {len(docs)} docs')\n",
    "    \n",
    "    insert_many_tc_docs(docs, coll)\n",
    "    docs = []\n",
    "print(f'attempted {totalDocs} inserts')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python argo",
   "language": "python",
   "name": "argo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
